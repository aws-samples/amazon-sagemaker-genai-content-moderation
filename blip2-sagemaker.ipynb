{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d942e395-2ebf-400d-8c0d-aaed37da9672",
   "metadata": {},
   "source": [
    "## Deploy BLIP2 Endpoint on SageMaker\n",
    "\n",
    "In this notebook, we will deploy a BLIP2 endpoint with DJLServing container image.\n",
    "\n",
    "This notebook has been tested within SageMaker Studio Notebook environment Python3 Data Science environment. \n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6d006-3e03-4773-810f-1e593c757b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ecae5-822a-487c-a58f-3a6f364eb508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import json\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc4d04-25f9-4c10-aadf-a76b57ba8cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e63f5-ab27-4eb1-81ca-2e166483f1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = \"blip2\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = \"model_blip2\"  # folder within bucket where code artifact will go\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "# define a variable to contain the s3url of the location that has the model\n",
    "pretrained_model_location = f\"s3://{model_bucket}/{s3_model_prefix}/\"\n",
    "print(f\"Pretrained model will be uploaded to ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7499bb-2944-4bdc-95a3-fd3ec3dee465",
   "metadata": {},
   "source": [
    "## Prepare inference script and container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee875d-91d0-4ab8-901f-0a0bce95652b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=sess.boto_session.region_name, version=\"0.22.1\"\n",
    ")\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53680769-4e06-4270-b376-eb3d5a71f3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blip_model_version = \"blip2-flan-t5-xl\"\n",
    "model_names = {\n",
    "    \"caption_model_name\": blip_model_version, #@param [\"blip-base\", \"blip-large\", \"blip2-flan-t5-xl\"]\n",
    "}\n",
    "with open(\"blip2/model_name.json\",'w') as file:\n",
    "    json.dump(model_names, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c3412-1df2-49d2-9cd0-31b2326389c4",
   "metadata": {},
   "source": [
    "In this notebook, we will provide two ways to load the model when deploying to an endpoint.\n",
    "- Directly load from Hugging Face \n",
    "- Store the model artifacts on S3 and load the model directly from S3\n",
    "\n",
    "The [Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html) container uses [s5cmd](https://github.com/peak/s5cmd) to download data from S3 which significantly reduces the speed when loading model during deployment. Therefore, we recommend to load the model from S3 by following the below section to download the model from Hugging Face and upload the model on S3. \n",
    "\n",
    "If you choose to load the model directly from Hugging Face during model deployment, you can skip the below section and jump to the section to **prepare the model tarbal file and upload to S3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7102d7-58e5-449e-8dc3-eb1bf76da79a",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "If you intend to download your copy of the model and upload it to a s3 location in your AWS account, please follow the below steps, else you can skip to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf38b0-5006-40e0-95be-8b1bbea48ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "CAPTION_MODELS = {\n",
    "    'blip-base': 'Salesforce/blip-image-captioning-base',   # 990MB\n",
    "    'blip-large': 'Salesforce/blip-image-captioning-large', # 1.9GB\n",
    "    'blip2-2.7b': 'Salesforce/blip2-opt-2.7b',              # 15.5GB\n",
    "    'blip2-flan-t5-xl': 'Salesforce/blip2-flan-t5-xl',      # 15.77GB\n",
    "}\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\"./blip2-model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = CAPTION_MODELS[blip_model_version]\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\", \"*.safetensors\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2be006-576f-4799-a16c-f26a9da66b80",
   "metadata": {},
   "source": [
    "Please make sure the file is downloaded correctly by checking the files exist in the newly created folder `blip2-model/models--Salesforce--<model-name>/snapshots/...` before running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0aeb4-9e76-4b4b-ab69-f3aacecb32b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload the model artifacts to s3\n",
    "model_artifact = sess.upload_data(path=model_download_path, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")\n",
    "print(f\"We will set option.s3url={model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8532e-ee56-4596-a55a-0666baac42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {local_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823c93a-4bc8-44f9-b4c2-b8f137e9c46d",
   "metadata": {},
   "source": [
    "SageMaker Large Model Inference containers can be used to host models without providing your own inference code. This is extremely useful when there is no custom pre-processing of the input data or post-processing of the model's predictions.\n",
    "\n",
    "However, in this notebook, we demonstrate how to deploy a model with custom inference code.\n",
    "\n",
    "SageMaker needs the model artifacts to be in a Tarball format. In this example, we provide the following files - `serving.properties`, `model.py`, and `requirements.txt`.\n",
    "- `serving.properties` is the configuration file that can be used to indicate to DJL Serving which model parallelization and inference optimization libraries you would like to use. Depending on your need, you can set the appropriate configuration. For more details on the configuration options and an exhaustive list, you can refer the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html).\n",
    "- `model.py` is the script handles any requests for serving.\n",
    "- `requirements.txt` is the text file containing any additional pip wheel need to install. \n",
    "\n",
    "If you want to download the model from huggingface.co, you can set option.model_id. The model id of a pretrained model hosted inside a model repository on huggingface.co (https://huggingface.co/models). The container uses this model id to download the corresponding model repository on huggingface.co. If you set the model_id to a s3 url, the DJL will download the model artifacts from s3 and swap the model_id to the actual location of the model artifacts. In your script, you can point to this value to load the pre-trained model.\n",
    "- `option.tensor_parallel_degree`: Set to the number of GPU devices over which the model needs to be partitioned. This parameter also controls the number of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine, and we are creating 8 partitions then we will have 1 worker per model to serve the requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f38dc-1d75-4167-a4f9-4518024c7bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile blip2/serving.properties\n",
    "engine = Python\n",
    "option.tensor_parallel_degree = 1\n",
    "option.model_id = {{s3url}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eabd92-12d6-4f05-90fe-219c290e0b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(\"blip2/serving.properties\").open().read())\n",
    "Path(\"blip2/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3url=pretrained_model_location)\n",
    ")\n",
    "!pygmentize blip2/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c66dd-ee1d-4cc5-961b-0b9b063c4f22",
   "metadata": {},
   "source": [
    "## Prepare the model tarball file and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae8bc2-f3c9-4174-8ebe-3230efaa72a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "tar czvf model.tar.gz blip2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0e818-1a0a-4d5a-b86e-f06d19eaa0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fb573-b45c-4bbc-8f2e-0e46bff6f14a",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c8205-c549-4f09-abab-7525047fab39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(blip_model_version)\n",
    "model = Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=s3_code_artifact,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc1232-8d70-4ea1-a617-d43fb4eaab3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "endpoint_name = \"endpoint-\" + model_name\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1b633-2839-42c4-a4e0-65568e8bd302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af32882-3587-4832-9948-36d36d87b050",
   "metadata": {},
   "source": [
    "## Test Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987ea64-7038-4020-8144-8895fd072b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "endpoint_name = model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae73ace-0aea-4379-9083-471b168d62bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_image(img_file):\n",
    "    with open(img_file, \"rb\") as image_file:\n",
    "        img_str = base64.b64encode(image_file.read())\n",
    "        base64_string = img_str.decode(\"latin1\")\n",
    "    return base64_string\n",
    "\n",
    "def run_inference(endpoint_name, inputs):\n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, Body=json.dumps(inputs)\n",
    "    )\n",
    "    return response[\"Body\"].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f152b-dba5-4996-8813-120708655f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image = \"carcrash-ai.jpeg\"\n",
    "raw_image = Image.open(test_image).convert('RGB')\n",
    "display(raw_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0f164-fe65-4d49-8787-92f7e17fe327",
   "metadata": {},
   "source": [
    "### Image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9581283f-98ae-4209-8ccf-6d041e57dd74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base64_string = encode_image(test_image)\n",
    "inputs = {\"image\": base64_string}\n",
    "run_inference(endpoint_name, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550a4af-9202-4525-b8f9-7047edd3ba4c",
   "metadata": {},
   "source": [
    "### Instructed zero-shot vision-to-language generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81128f3-1f0b-432b-a82d-ae6dc98615bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base64_string = encode_image(test_image)\n",
    "inputs = {\"prompt\": \"Question: what happened in this photo? Answer:\", \"image\": base64_string}\n",
    "run_inference(endpoint_name, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2e11b-7cee-4790-967a-100084039a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = [\n",
    "    (\"what happened in this photo?\", \"a car accident\"),\n",
    "]\n",
    "question = \"which parts are damaged in this car?\"\n",
    "template = \"Question: {} Answer: {}.\"\n",
    "\n",
    "prompt = \" \".join([template.format(context[i][0], context[i][1]) for i in range(len(context))]) + \" Question: \" + question + \" Answer:\"\n",
    "\n",
    "length_output_params = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"min_new_tokens\": 5,\n",
    "    \"early_stopping\": True\n",
    "}\n",
    "\n",
    "# Parameters that control the generation strategy used\n",
    "gen_strategy_params = {\n",
    "    \"do_sample\": False,\n",
    "    \"num_beams\": 2,\n",
    "    \"num_beam_groups\": 1,\n",
    "    \"use_cache\": True\n",
    "}\n",
    "\n",
    "gen_strategy_params.update(length_output_params)\n",
    "\n",
    "inputs = {\"prompt\": prompt, \"image\": base64_string, \"parameters\": gen_strategy_params}\n",
    "run_inference(endpoint_name, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c714b1b-00c6-48be-b0d0-2381bf5e6cf2",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Uncomment the below cell to delete the endpoint and model when you finish the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22616f1c-a919-4f77-af58-de3a0bbffe06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sm_client.delete_model(ModelName=model_name)\n",
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7d3ff-5ae5-4aa1-9f29-bf71abe4dcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
